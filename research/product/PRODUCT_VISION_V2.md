# AthenaMind Product Vision — v2.0 and Beyond

## Status
Draft — captured from founder working session 2026-02-23. Not in scope for v0.1 execution. Informs long-term product direction and business model.

## Scope Boundary
This document describes the long-term product vision. Current execution scope remains v0.1 memory-layer only (ADR-0007). Agents should not treat this as implementation guidance for current sprint work.

---

## Core Insight
Agents waste 25%+ of their context window on orientation — figuring out what to do, where things are, and how this codebase/org works. That cost repeats every session because agents are stateless. Good engineering practices (tight scoping, clear interfaces, structured handoffs) reduce this cost, but only if they're encoded explicitly.

AthenaMind externalizes this knowledge into a governed memory layer that gives agents (and humans) instant access to the right context at the right time.

## The Memory Hierarchy

### Skill Packs
Reusable procedural knowledge loaded by context, analogous to human expertise domains.

- **Core skills** — always loaded at bootstrap (cycle protocol, handoff format, orientation pattern). Small, stable, fits in any context window.
- **Domain skills** — loaded by context match (Go patterns, SRE runbooks, data pipeline conventions, testing strategies). Retrieved semantically based on the task at hand.
- **Repo skills** — loaded per codebase (build commands, architecture, conventions, entry points). Generated by first-agent exploration, consumed by every subsequent agent.
- **Episode memory** — loaded on demand (what happened last session, what failed, what was decided and why). Queryable state that compounds across sessions.

Agents have an advantage over humans: no decay, no rustiness, perfect recall if the memory exists. The bottleneck is having the right skill pack written and indexed — not remembering it.

### The O(1) Orientation Protocol
Instead of agents exploring a repo to figure out what to do (O(n) with codebase size), the memory system provides constant-cost orientation:
1. Load core skill (how to operate)
2. Query launcher (what to work on)
3. Retrieve repo/domain skills (how this codebase works)
4. Retrieve episode context (what happened last time)

First agent pays the exploration cost once. Every agent after that pays near-zero.

## Two Consumers, One Memory

The same memory store serves both machines and humans:

- **Agent asks:** "How does this org do DI in C#?" → gets an executable skill pack it can follow
- **Human asks:** the same question → gets an explanation with examples from their actual codebase, grounded in how their org does it — not how the internet says to do it

Both answers come from the same governed memory, built from real code, real decisions, and real patterns. Not Stack Overflow. Not a stale wiki. The actual institutional knowledge of the organization.

### Organizational Knowledge as a Side Effect
Every agent session that writes an episode, every decision record, every codebase bootstrap — they all add to the knowledge base. Agents aren't just doing work; they're documenting the organization as a side effect of working. The knowledge base compounds automatically.

## The Onboarding Problem

A principal SRE joining a new org spends months absorbing: coding standards, architectural patterns, deployment conventions, incident response procedures, which approaches have been tried and failed, who owns what. This knowledge is scattered across repos, wikis, Slack, Confluence, runbooks, and people's heads. Search across these mediums is poor. Search across platforms is nearly nonexistent.

AthenaMind solves this from both directions:
- **Human ramp-up:** "Show me how we handle auth" → retrieves actual patterns from actual repos with actual decision history
- **Agent ramp-up:** same query → retrieves executable conventions it follows immediately

The year-long ramp-up that humans endure is the same exploration the memory system automates and persists.

## Layered Policy Hierarchy (Team/Org Scale)

When memory is centralized for a team or org, agents behave predictably according to the company's patterns:

- **Org level** — "We use trunk-based development, all services emit OTEL traces, PRs require two approvals"
- **Team level** — "This team owns the payments domain, we use event sourcing, our SLA is 99.95%"
- **Repo level** — "This service uses Go 1.22, build with make, deploys via ArgoCD"
- **Session level** — "Last cycle we migrated the auth endpoint, here's what changed"

Any engineer on any team launches an agent and it immediately behaves like a tenured team member — follows the right conventions, knows the architecture, respects the guardrails. No ramp-up period.

## Why Governance-First is the Moat

In an org context, governance isn't optional:
- You can't have a random agent session writing "we use MongoDB now" into org-level memory
- Enterprise procurement requires audit trails and compliance evidence
- Trust in the system depends on being able to verify: what was ingested, how it was classified, what confidence level, who reviewed it

The review-first model, risk classes, policy-gated writes, and audit trails that feel heavyweight for a solo operator become **essential infrastructure** for shared memory. Competitors who skipped governance can't retrofit it without rebuilding.

AthenaMind's audit chain provides "the receipts":
- Exactly what was ingested and from where
- How each entry was classified and indexed
- Confidence scores on every entry
- What was flagged for human review
- Full audit trail for every memory decision

## Enterprise Engagement Model: The Memory Engineer

A "memory engineer" is a services role that configures the ingest pipeline for an organization:

1. Points AthenaMind at the org's knowledge sources (repos, wikis, runbooks, Confluence, Slack archives, ADRs, incident postmortems)
2. A specialized version of the AthenaMind process crawls, sorts, indexes, embeds, and vectorizes
3. Human review gates flag low-confidence or sensitive entries for org approval
4. When complete, the org has a governed, auditable, queryable knowledge base
5. The audit trail proves exactly what was ingested and how — compliance-ready from day one

## Revenue Model

- **Open core** — local single-operator memory CLI (v0.1-v0.3, current build)
- **Team/org tier** — centralized memory store with shared retrieval and role-based governance
- **Enterprise services** — memory engineer engagement, custom ingest pipeline, org-specific skill packs, compliance reporting from the audit chain

The engineering work is the same across tiers. CLI, retrieval pipeline, governance gates, telemetry, adapter interfaces — all serve every tier. The variable is where the memory store lives and who can read/write.

## What v0.1 Builds Toward This

Every v0.1 primitive directly serves the long-term vision:

| v0.1 Primitive | Long-term Role |
| --- | --- |
| File-based memory store | Becomes the adapter interface for any backend |
| Governance gates | Becomes org-level write policy |
| Audit/telemetry | Becomes compliance reporting |
| Retrieval pipeline | Becomes skill pack retrieval |
| Bootstrap protocol | Becomes agent onboarding at any scale |
| Episode write-back | Becomes the compounding knowledge loop |
| Evaluation harness | Becomes retrieval quality SLA for enterprise |

## Risks to This Vision

1. **Retrieval quality must be excellent.** If agents retrieve wrong memories, the system erodes trust faster than it builds it. The precision-over-volume philosophy and quality gates are critical.
2. **Ingest quality at org scale is hard.** Corporate knowledge sources are messy, duplicated, contradictory, and stale. The ingest pipeline needs strong dedup, conflict detection, and staleness handling.
3. **Multi-tenant governance is complex.** Org/team/repo/session layering with inheritance and override rules is a significant design challenge.
4. **The market is moving fast.** Context window sizes are growing, which reduces the orientation tax. But organizational knowledge fragmentation grows faster than context windows — the problem compounds.

## Key Assumptions (Explicit)

- A1: Governance and traceability depth are under-served in competing memory offerings.
- A2: Organizational knowledge fragmentation is a growing problem that context window expansion alone won't solve.
- A3: The agent + human dual-consumer model creates more value than agent-only or human-only knowledge systems.
- A4: First-mover advantage in governed org-memory creates switching costs (the knowledge base itself becomes sticky).
- A5: The memory engineer services role is viable and scalable as a professional services offering.

## References
- Current v0.1 scope: `research/decisions/ADR-0007-memory-layer-scope-refinement.md`
- Competitive positioning: `research/competitive/COMPETITIVE_POSITIONING_AND_DIFFERENTIATION_MEMO_2026-02-22.md`
- Phased plan: `research/roadmap/PHASED_IMPLEMENTATION_PLAN_V01_V03.md`
- North star metrics: `research/decisions/ADR-0002-north-star-and-success-metrics.md`
- Governance model: `research/decisions/ADR-0006-governance-and-hitl-policy.md`
